{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"The Mooc V1 content is used by each partner during the pilot experimentation phase. It addresses the O1 objective of the AI4T project that is: To give teachers a basic to solid culture about AI allowing them to become first-class citizens as far as understanding AI, how it works, the challenges and consequences. AI4T project is co-funded by the European Union's Erasmus+ programme.","title":"Home"},{"location":"general-presentation/0-0-what-does-this-mooc-offer-us/0-0-0-why-this-mooc.html","text":"Used by everyone from now on, the term \"artificial intelligence\" needs to be explained and understood in order to be able to appropriate it and to take the necessary distance from the numerous preconceived ideas on the subject. The objective of this online resource is to share a minimal culture on Artificial Intelligence, or rather AI components, in particular in the school context, and to give everyone the possibility to better master the issues and the use of these technologies. A content based on IAI Class'Code Mooc This online ressource is essentially based on the content of the French Mooc \" L'Intelligence Artificielle... avec intelligence! \" available on Fun Mooc and produced in 2020 by Class'Code, Inria, Magic Maker, S24B l'Interactive, with the participation of 4minutes34, Data Bingo, the University of Nantes, La Compagnie du Code, La Ligue de l'enseignement and the support of the French Ministry of National Education and Youth, UNIT, EducAzur and leblob.fr.","title":"Why this Mooc?"},{"location":"general-presentation/0-0-what-does-this-mooc-offer-us/0-0-1-what-is-in-this-course.html","text":"The MOOC is organised in 3 Modules: Module 1: What is meant by Artificial Intelligence? This first part is dedicated to the acquisition of a first understanding of what is meant by Artificial Intelligence, what it is not and finally raises the question: What is intelligence ? Module 2: How does Machine Learning work? This second part helps to understand the basics of machine learning and the key role played by the control of data sets. Module 3: Artificial Intelligence at our service? This third part deals with the problems of ethics and aims to understand the stakes and the levers for AI to be at the service of humans. Each module is articulated in 3 steps: To Question Yourself: to move from preconceived ideas to questions on which to build understanding. To Experiment: to perform activity (on line or not) and see by yourself. To Discover: to understand how it works through the history of humans and their ideas. Videos and quizzes? About videos The Mooc offers several introducing short videos. Their objective is to arouse the participant's curiosity, make them want to know more, trigger a debate... The scenario features a main character, Guillaume, who is questioning and seeking to know more about AI. Sometimes he talks on the phone with his cousin Theo and calls a fictitious hotline. Participants should not be surprised by this non-academic style of the videos. About quizzes The Mooc offers quizzes at the end of each Module. Their objective is to reinforce the knowledge and skills acquired. Note : At the end of each quizz a passing score is displayed, which is a rating of your answers. This information is not taken into account in the Mooc pedagogical pathway (because there is no evaluation of learners).","title":"What is in this Course?"},{"location":"general-presentation/0-1-conditions-of-use-of-the-mooc-and-its-contents/0-1-0-conditions-of-use.html","text":"During the pilot phase the content should not be shared outside the consortium members. The conditions of use after the pilot and experimentation phases of the final ressources are now discussed within the AI4T project.","title":"Conditions of use of the content of the Mooc"},{"location":"module-1-what-is-meant-by-ai/1-0-introduction/1-0-0-presentation.html","text":"Objective of Module 1: Gain an initial understanding of what is meant by Artificial Intelligence (AI) and what it is not. Content: To question: Who Is Afraid of AI? To Experiment: Let's Test Our First Program To Discover: What Is Artificial Intelligence? Quiz Module 1: Who is Afraid of AI? Investment time: About 30 minutes","title":"Presentation"},{"location":"module-1-what-is-meant-by-ai/1-1-to-question-who-is-afraid-of-ai/1-1-0-who-is-afraid-of-ai.html","text":"Artificial intelligence! The more I hear about it, the less I understand it... I thought it was top, but now I see an article where Stephen Hawking, you know the hyper brilliant researcher, who says: \"the development of full artificial intelligence could end the human race\". Do you also have the same question? Watch the video below (3'04\") Synopsis The main character \u00ab Guillaume \u00bb asks himself questions about artificial intelligence and its implications on our lives. Through an interaction with an off-screen character \u00ab The Hotline \u00bb, general questions about AI are raised, prior to further questioning. It is now reminded that AI is first and foremost a scientific discipline.","title":"Who Is Afraid of AI?"},{"location":"module-1-what-is-meant-by-ai/1-1-to-question-who-is-afraid-of-ai/1-1-1-did-you-say-intelligence.html","text":"Translated text from the french IAI Mooc. More than being unbeatable at mental arithmetic or Go, intelligence is the ability of a living system to understand, interpret, learn and adapt to change. To be intelligent is to know how to find the most appropriate response to a problem , for which we rely on all our mental and cognitive faculties. Moreover, intelligence is embodied, it is something inseparable from our body. If we take the example of the game of Go, we use our intellect to adapt our game strategy to a situation, anticipate that of our opponent and find the best combination to win the game. But then, if in 2016 the Alpha Go program managed to beat the Korean Go champion, does that mean that computer programs are more intelligent than human intelligence? In short, what is the difference between the intelligence of a human being and that of a machine ? While artificial intelligence is very impressive, it is still limited to a well-defined domain. Moreover, it is not embodied, unlike our biological intelligence embodied in a body. In the case of machine learning technologies, \"intelligence\" is based on learning and to do this it needs to make statistics on a lot of data, unlike our intelligence which can make relevant deductions from a few examples. Of course the machine is capable of performing calculations and processing information at a breakneck pace, but the machine does not understand the task you are asking it to perform. For example, if you ask a child to look for a picture of a dog in a picture book, the child will only need to see one or two pictures of a dog to be able to recognise the animal, even in an unusual situation (at night, for example). The child, beyond visualising what a dog might look like, will then be able to define and describe it, and even evoke possible emotional ties that he or she might have developed with the animal. An algorithm, on the other hand, will need hundreds of thousands of photos before it can recognise a dog without making a mistake. On the other hand, if you launch a search on the Internet with the word \"dog\", the search engine will be able to display hundreds of millions of images of dogs, without knowing what a dog is, nor defining it, nor describing it, nor explaining how it could have felt any emotion towards it. Human or animal intelligence, biological intelligence, is based on cognitive and also emotional capacities, linked to the body. A so-called \"strong\" artificial intelligence, which would be able to be autonomous and versatile in unexpected situations, is a scientific objective. However, at present, there are results that show that this ideal goal of strong artificial intelligence is technically impossible. For the moment, this is a belief, not a future scientific revolution.","title":"Did You say Intelligence?"},{"location":"module-1-what-is-meant-by-ai/1-1-to-question-who-is-afraid-of-ai/1-1-2-a-brief-history-of-ai.html","text":"Translated text from the french IAI Mooc. Today, artificial intelligence is present in many of the technologies we use every day: in our computers, our mobile phones, our watches, our speakers... It is even present in our search engines and on social networks. This is probably what explains the new craze that it is experiencing today, we are passionate about it because it has enabled us to boost our technologies and therefore have better uses for them. However, AI is an old scientific discipline , it was officially recognised as a field of research in 1956 and the progress of research in this field will be booming until 1974. Subsequently, as the hoped-for results did not materialise, investors lost interest in the discipline and research in the field began to falter until 1980: this was the first winter of AI. It was the rise in the 1980s of expert systems , which made it possible to reproduce cognitive abilities and to outperform experts in their field, that revived the dynamics of the discipline. But here again, the enthusiasm of funders is waning in the face of slower-than-expected progress, and AI will experience a second winter of about ten year In the mid-1990s, AI experienced a new boom, propelled - among other things - by the victory of IBM's Deep Blue programme over the chess champion Garry Kasparov in 1997. New performances, for example in image recognition, are at the origin of the current enthusiasm for the discipline. Current achievements in artificial intelligence, such as the development of voice assistants, medical imaging analysis or intelligent mechanisms in cars, are the result of the research advances of the last twenty years. These technological feats, which can be improved, will undoubtedly have an impact on our societies in the long term and will be the source of transformations in many areas such as health, justice, the media and transport. However, these developments are relative, and we must adopt a technocritical attitude towards their applications.","title":"A Brief History of AI"},{"location":"module-1-what-is-meant-by-ai/1-1-to-question-who-is-afraid-of-ai/1-1-3-should-we-be-afraid-of-ai.html","text":"Translated text from the french IAI Mooc. AI makes us dream as well as have nightmares. It must be said that it has greatly inspired science fiction, from \"2001 : A Space Odyssey\" to \"Terminator\" or more recently \"Her\", there is plenty to feed our imagination. However, many eminent scientists, such as Stephen Hawkings and Elon Musk, also warn us against the development of this discipline. So should we fear that AI will destroy humanity, or on the contrary should we see it as our salvation? \u00ab The Starry Night \u00bb by Vincent Van Gogh as seen by Google Deep Dream Of course, the reality is much less Manichean. First of all, we need to recontextualise things. Part of the fantasy about artificial intelligence stems from the collective imagination we have built up around it, i.e. that of super-intelligent machines or programmes capable of one day surpassing human beings; it's the good old myth of the singularity. In reality, AI is not a technology, but a scientific discipline with its own technological advances. Today, programmes based on AI are more about \"knowledge\" through learning than real intelligence . This raises questions about our ethics, particularly because, as we have seen, AI needs data to learn, and these are often personal data. So we should not get carried away, we are still a long way from programs that think for themselves (and besides, maybe we will never get there). It is also necessary to recontextualise the concern of researchers and in particular the AI Open Letter of January 2015 written by a dozen scientists. While in this letter, the researchers warn of the potential risks of the development of AI, they also highlight the positive effects that it could have on humanity . It is therefore in fact more informative than alarmist and aims above all to integrate ethical and societal issues into the heart of the debate. It is with this in mind that the international consortium Partnership of AI was created, bringing together large companies (such as GAFA) as well as associations, NGOs and universities to ensure that the impact of AI on human societies remains at the heart of the debate. Ultimately, the definition of AI lies between myths, fantasies, scientific and technical innovations , but for now, let's leave the myth of the singularity to fiction and focus on the scientific advances that already exist.","title":"Should We Be Afraid of AI?"},{"location":"module-1-what-is-meant-by-ai/1-2-to-experiment-lets-test-our-first-program/1-2-0-tutorial-lets-test-our-first-program.html","text":"What is AI and how does an AI program work? Go ahead and train an AI! How to experiment? Click on the image below and let yourself be guided!","title":"Let's Test our First Program"},{"location":"module-1-what-is-meant-by-ai/1-3-to-discover-what-is-artificial-intelligence/1-3-0-what-is-artificial-intelligence.html","text":"\"Our computers, have they become intelligent or not?\" Let's see what biological intelligence is so we can define AI with the different symbolic, numerical approaches... All the artificial intelligences we come across in real life are just technical and very specific! Want to find out \"what is artificial intelligence?\" Watch the video below (8'22\") Synopsis The main character Guillaume lists the many applications or tools that use algorithms and highlights their place in our daily lives. He starts to question computers and computer science which leads him to ask the question: \"Have our computers become intelligent or not?\". Guillaume then turns his interest to biological intelligence, whose functioning allows him to ask questions about the \"real intelligence\" of AI and its different approaches: symbolic, digital...The conclusion of the video is that all the artificial intelligences we encounter in everyday life are restricted to specific tasks, far from the representations we can have in movies or in fiction!","title":"What is Artificial Intelligence?"},{"location":"module-1-what-is-meant-by-ai/1-3-to-discover-what-is-artificial-intelligence/1-3-1-what-types-of-AI.html","text":"Many AI definitions and classification can be found in the litterature. Although AI has entered the common language, there is no really shared definition. One of the classification distinguishes between weak AI and strong AI. An other one refers to symbolic or numerical approaches to artificial intelligence. Let's see what do they refer to. Translated text from the french IAI Mooc. Weak or strong AI? The expression \"artificial intelligence\", created in the 1950s, refers to the field of research that studies the mechanisms of intelligence by modelling them with algorithms and experimenting with machines. These mechanisms include, for example, the ability to automatically find solutions to problems, which may involve planning, prediction, control, memory or learning. By extension, the term \"artificial intelligence\" is often used to refer to algorithms that simulate or share some of the intelligence capabilities of living beings. Weak artificial intelligence This is the artificial intelligence we know today: it is an algorithm that \"learns\", by adapting its parameters to learning data, and is not endowed with mental and cognitive capacities, but is capable of performing a specific task much more efficiently, sometimes more so than a human being. Strong artificial intelligence An artificial intelligence that would be capable of copying human aptitudes (learning, understanding, apprehending, reasoning, making decisions, having a conscience, emotions, etc.). To date, strong artificial intelligence does not exist, it is a belief. Symbolic or numerical approach? Let's imagine that we want to program a kitchen robot to make a pot au feu, we can: have a symbolic approach , i.e. based on logic and a priori knowledge. If you are a specialist in this recipe, you can explicitly give a series of rules and principles for making a pot au feu: this will be very good if you are a great cook and can even propose very high level techniques, with complicated reasoning. have a numerical approach , i.e. based on data and learning. Without knowing too well how it is done, but by observing many other people who make pots on fire, we will be able to draw general rules from a statistical and therefore numerical analysis of their methods and propose solutions. This will certainly be less explicit than with knowledge already acquired, but it will be more flexible and adaptive because it will be based on the diversity of observations. It might even be possible to find things that no one has experienced before (for example, that some people put the meat in before or after heating the water). Each of these two approaches makes it possible to make a machine perform functions that would have been considered intelligent if done by a human: you are an expert and you formalise and specify your knowledge or you observe and imitate without having any a priori knowledge, trying to make this knowledge emerge by analysis and comparison of similar cases, by learning. This corresponds to two different faculties of the brain: I know that (explicit memory) or I know how to do (or rather I learn to do by experience, implicit memory). Historically, the symbolic approach is older, corresponding to expert systems, and more recently to the so-called semantic web. By formalising our knowledge in the form of ontologies, its great advantage is to be able to introduce specific knowledge a priori, before applying rules to automate a certain amount of reasoning. It also has the merit of being able to be used even with little data. It is based on formalisations of logic. The numerical approach corresponds, for example, to what is known as artificial neural networks, deep learning when there are several layers of such computing units. It has become effective more recently, and it is this approach that allows us to automatically transcribe texts that we dictate, or to recognise objects in images. It requires a lot of data and is based on statistical approaches. It is a current research topic to see how to combine the two approaches. A Glossary about AI is published by the Council of Europe . Some more AI related terms are defined.","title":"What types of AI?"},{"location":"module-1-what-is-meant-by-ai/1-3-to-discover-what-is-artificial-intelligence/1-3-2-the-ai-of-outstanding-achievements.html","text":"AI has become a hot topic in the media and science magazines due to numerous achievements, many of which are the result of advances in the field of machine learning. AI research has made significant progress in the last decade in a variety of areas. The best known advances are those in machine learning , thanks in particular to the development of deep learning architectures, multi-layer convolutional neural networks that learn from large volumes of data on intensive computing architectures. Some notable examples: solving Atari games (Bricks, Space invaders, etc.) by Google DeepMind, using the pixel images displayed on the screen as input to decide what action to take to achieve the highest possible score at the end of the game the victory in the game of Go , by the same team, over the best human player in a series of five games automatic description of the content of an image (\" An image speaks louder than a thousand words \"), also by Google the results of Imagenet's Large Scale Visualisation Challenge 2012 , won by a very large convolutional neural network developed by the University of Toronto, and trained with Nvidia GPUs the quality of facial recognition systems such as those of Facebook aso. But this is only a very small part of what AI has achieved. The advances in robotics, autonomous vehicles, speech processing and natural language understanding are equally impressive . Examples include: the skill level achieved by robots in Darpa's Robotic Challenge , won by KAIST in 2015. In this competition, robots must drive a utility vehicle, open a door and enter a building, close a valve, use a tool to drill through a wall, move through rubble or remove debris blocking an entrance door and climb a ladder; speech understanding. It is now considered a standard feature of smartphones and tablets with artificial companions such as Apple's Siri, Microsoft's Cortana or Facebook's M; Instant translation. Microsoft Skype Translator translates conversations in different languages in real time; driverless vehicles. Autonomous cars have travelled thousands of kilometres without major incidents. Equally fundamental are the results obtained in areas such as knowledge representation and reasoning, ontologies and other technologies for the semantic web and the web of data: Google Knowledge Graph improves search results by displaying structured data on requested search terms or phrases; Schema.org contains millions of RDF triples describing known facts: search engines can use this data to provide structured information on demand; Facebook uses the **OpenGraph protocol - based on RDFa - to allow any web page to become an enriched object in a social graph. Finally, another important trend is the recent opening up of several previously proprietary technologies so that the AI research community can not only benefit from them, but also contribute to enriching them with new features. Examples include: IBM Watson's cognitive services, available on programmatic interfaces (APIs), which offer up to twenty different technologies such as Speech To Text and Text To Speech services, concept identification and linkage, visual recognition and many others; Google's TensorFlow Software Library used for machine learning, which has been made open source; Facebook's release of its open source Big Sur server design for running large deep learning neural networks on GPUs The enthusiasm generated by all these positive achievements has, however, been tempered by the concerns expressed here and there by leading scientists. Source: \"Artificial intelligence, current challenges and Inria's engagement\" - Inria white paper, 2016.","title":"The AI of Outstanding Achievements"},{"location":"module-1-what-is-meant-by-ai/1-4-quiz-module-1/1-4-0-quiz1-who-is-afraid-of-ai.html","text":"QUIZ Module 1: Who is Afraid of AI? Quiz Module 1 contains 3 parts: QUIZ 1: Did you Say AI? QUIZ 2: A Bit of History QUIZ 3: Various Approaches of AI Click on the image below and start the quiz!","title":"Who is Afraid of AI?"},{"location":"module-2-how-does-ml-work/2-0-introduction/2-0-0-presentation.html","text":"Objective of Module 2: Understand the principles of machine learning and the crucial role played by the control of data sets. Content: To Question: AI in all its forms ? To Experiment: Let's play with the data To Discover: Machine learning and data Quiz Module 2: Mechanisms in AI Investment time: about 50 minutes","title":"Presentation"},{"location":"module-2-how-does-ml-work/2-1-to-question-ai-in-all-its-forms/2-1-0-ai-in-all-its-forms.html","text":"How do they put artificial intelligence in a pizza? Want to know how it's possible?\" Watch the video below (1'26\") Synopsis The main character, Guillaume , discovers new and unexpected uses of AI, such as the ability to create a pizza recipe using machine learning. The video then focuses on the place of algorithms and data in many fields, including marketing.","title":"Boosted with AI!"},{"location":"module-2-how-does-ml-work/2-1-to-question-ai-in-all-its-forms/2-1-1-our-daily-lives-boosted-by-ai.html","text":"Texte translated from IAI Mooc. Many of the technologies we use now work with artificial intelligence. They use two types of AI software applications: a \"recognition\" system and/or a prediction system. \u00a9 Inria / Photo G. Cohen A recognition system identifies and classifies all the data submitted to it. It can recognise an image, a face, a sound, a word, etc. Such a system increases the functionality of our smartphone applications, search engines or social networking software. Thanks to recognition systems, an application can now identify a person's photos and classify them in a dedicated album. This is also what allows search engine translators such as Google to automatically identify the original language of a word that is being translated. The predictive system , on the other hand, makes it possible to predict future events or behaviour, such as the weather forecast, based on a mass of data. It is, for example, widely used on social networks and online sales sites to target advertisements to users. Based on your data (age, gender, place of residence, interests, recent online purchases, recent searches, etc.) the algorithm determines the purchases you are most likely to make in the coming days and offers you advertising based on this data. Depending on the way we use these AI technologies, it may be more of a \"gimmick\" than a real \"innovation\". But in many areas it has led to impressive technical progress. This is what is presented in the book \"Artificial Intelligence - Investigating the technologies that are changing our lives\" 1 . Among other things, these tools are set to revolutionise the fields of: Medicine: we are moving towards what the Leroy Hood Institute defined in 2013 as \"P4 medicine\". That is to say, medicine that would be \"preventive\", \"participative\", \"personalised\" and \"predictive\". Some of the new technologies are simple bots that can make a pre-diagnosis without any examination, just by looking at people's digital traces... Other technologies allow doctors to better detect breast cancer. This is crucial because when breast cancer is diagnosed in its early stages, it has a better chance of being cured. The media: short articles can now be produced automatically. Far from being able to claim a press card, this simplistic technology, which produces texts devoid of journalistic style and analysis, perhaps shows that many human-written articles are ultimately no more sophisticated. Nevertheless, the technology is a time-saver, and some media outlets have used the programme to produce texts on the 2014 and 2015 municipal and departmental elections in France to make up for staff shortages. Justice: AnaCrim data analysis software has been helping the French gendarmerie for many years. Made famous in 2017 when it was used in well known case in France of child assassination, this software is used when a criminal investigation includes many elements to be analysed and dated, to provide elements to assist human decisions. The creative arts (cinema, music, film, etc.): whether for composing music or enhancing cinematic special effects, algorithms have long been present in the arts and culture. Artificial intelligence mechanisms are also used in cooking, to predict recipes that are more likely to please customers or to create new tastes and flavours. These new technologies, boosted by artificial intelligence, are already enabling new technical and scientific progress and these algorithms are also widely used in research. However, the use of our data in these technologies and the potential risks linked to the intrusion into our private lives and to mass surveillance must be questioned and debated. Tasting a pizza prepared by an algorithm, why not, but can we accept to let an algorithm decide the amount of our credits or the judgment of a court case, without human discernment? 1 Reference in French : Intelligence artificielle : enqu\u00eate sur ces technologies qui changent nos vies [Ouvrage collectif] / Enki Bilal, Laurence Devillers, Gilles Dowek, Jean-Gabriel Ganascia et al. . 272 p. - Paris: Flammarion, 2018. ( AI :an investigation into the technologies that are changing our lives )","title":"Our Daily Lives Boosted by AI"},{"location":"module-2-how-does-ml-work/2-1-to-question-ai-in-all-its-forms/2-1-2-ai-how-does-it-work.html","text":"Texte translated from IAI Mooc. Many artificial intelligence mechanisms today work through supervised learning. This mechanism resembles one of the ways in which a biological system learns. Let's imagine that we want to teach an artificial intelligence to recognise a cat in an image. To do this, we will provide a lot of data, namely, many images where a cat can be seen and many images where no cat can be seen, so that the calculation adjusts its parameters in order to give an output value corresponding to the presence or not of the feline. All these images constitute the input data, and the expected result, whether or not there is a cat in the image, the output data. These \"input\" and \"output\" data are the only information provided for its training. The computational mechanism must therefore adjust internal parameters (such as the control knobs of a camera) to determine whether or not there is a cat in the image. The first time, a random and therefore most likely false result will be provided, then little by little the mechanism will observe the errors and by successive trials will adjust the parameters to reduce them. This process is known as machine learning. Image by Gerd Altmann from Pixabay This mechanism of artificial neural networks is quite different from the neurons in our brain: they are just elementary computational units that combine input data and deliver a low or high value as output depending on the combined value. A neural network is a set of neurons that are all connected and communicate with each other. It is the parameters of these connections between neurons that constitute the control knobs to obtain the desired output for a given input. From the input neurons to the output neurons, via the neurons inside the network (hidden neurons), the digital information is transmitted to give a final result. We speak of deep learning when there are many hidden layers stacked up to make the calculation more efficient. We speak of convolutional neural networks when neurons gather information from other neurons in the vicinity before transmitting the output to higher layers. For example, when analysing an image, a convolutional neural network will create filters to group information from a small area of the image. For example, the contrast, or a colour element, and then at the next layer small areas corresponding to basic elements such as a line or a slightly round area, and little by little the mechanism recognises a cat as well as Jim Davis, it's amazing. But then, what reasoning has been put in place to distinguish between images with and without cats? Well, none! It's just a calculation, a blind calculation in a way. And we don't know how to interpret such a calculation today. This is what we call the black box of AI 2 . 2 References in French: Comprendre le DeepLearning et les R\u00e9seaux de neurones en 10 mins ! vid\u00e9o, Sociamix, ao\u00fbt 2019 - (Understanding deep learning and neural networks in 10 mins!) Le deep learning , vid\u00e9o, Science \u00e9tonnante #27, avril 2016 Comment le \u00ab deep learning \u00bb r\u00e9volutionne l'intelligence artificielle par Morgane Tual ; Le Monde, juillet 2015. - (How Deep Learning transforms AI.)","title":"AI&#58; How Does it Work?"},{"location":"module-2-how-does-ml-work/2-2-to-experiment-lets-play-with-the-data/2-2-0-tutorial-boosted-with-ai.html","text":"There is no better way to understand how machine learning and program training works than to understand how to properly prepare your dataset. It's your turn! Click on the image below and let us guide you!","title":"Boosted with AI?"},{"location":"module-2-how-does-ml-work/2-2-to-experiment-lets-play-with-the-data/2-2-1-tensorflow-playground.html","text":"The online software TensorFlow allows to build artificial neural networks and to test their responses for different types of problems and on different types of data. In the \"Classification\" problem type, the objective is to separate blue and orange coloured points. An application of this type of problem is, for example, a photo classification algorithm. In the example below, there is one input (feature) that separates the points horizontally and another that separates vertically. By combining these two inputs, we obtain an oblique separation. The result (output) is well adapted to the type of data chosen. TensorFlow: Some explanations before trying the simulation of a neural network Source: translation from Pixees French web site What is a neural network and how does it work? A neural network is a generic mechanism composed of small units (pseudo-neurons) connected to each other. Each unit performs a very simple operation: it takes input values, combines them very simply (a simple averaging with coefficients) and applies a transformation on the result (e.g. keeps only positive values). The coefficients used to weight the average are the parameters of this algorithm. It is the combination of a very, very large number of these units that allows very complex operations to be performed. A network of such \"neurons\" is obtained by accumulating several layers of such units. The input is the data that we want to process. They are transformed through all the layers and the last layer gives as output a prediction on this data, for example to detect if there is a face in an image. The neural network is thus a parameterized function with many coefficients (called \"weights\") and it is the choice of these weights that defines the processing carried out. Where are the neurons in TensorFlow? On the TensorFlow web interface, a network of a dozen neurons, each with between 3 and 10 parameters, can easily be created. The calculated output thus depends on hundreds of parameters in addition to the two coordinates (x,y) of the input point. On the interface, each square represents a neuron and the colour of the pixel of coordinates (x,y) in the square represents the output of the neuron when we put (x,y) in input of the network. If there is only one neuron at the output, it is represented with a larger square on the right of the network. The parameters of the network are initialized with random values. But how do you learn these weights? Supervised learning consists of providing examples of data with the solution to be found in order to train the network to adjust these weights as required. In the example in the figure above, it is a series of points in a square, each with an expected colour (blue or orange), with the aim of predicting the colour of the point at a given location. A classical algorithm of progressive adjustment of the weights is used to find the parameters in question. The \"play\" button at the top left of the interface is used to launch this algorithm, and the output of the neural network is then seen to evolve during the \"learning\" process: the background colour of the output neuron tends to take on the colour of the training points that are drawn over it. Another part of the dataset is then used to test the quality of the resulting function of the network. A curve at the top right shows the error rate of the data used for learning (to check that the weights have adjusted properly) and the error rate of the other test data (to check that what has been learned generalises well to new data). Buttons on the left side allow to adjust the distribution of the data between the training and the test set and also to add errors to the data (noisy data) to see if the mechanism is robust to these errors. In practice, we manage to find satisfactory parameters, but there is no real theoretical framework for formalising all this, it's a matter of experimentation: choosing the right number of neurons, the right number of layers of neurons, what preliminary calculations to add as input (for example multiplying the inputs to increase the degrees of freedom for the calculation). These kinds of techniques can produce impressive results in practice, such as in voice or object recognition in an image. However, understanding why (and how) such good results are obtained is still a fairly open scientific question. Try TensorFlow Click on the image below to access the TensorFlow application in a new window","title":"Play with the Machine's Neurons"},{"location":"module-2-how-does-ml-work/2-3-to-discover-of-machine-learning-data/2-3-0-of-machine-learning-and-data-video.html","text":"When we talk about AI today, we most often hear \"machine learning\". But how do machines learn, and what do they learn from? Explanation Watch the video below (9'35\") Synopsis The video presents the different approaches to learning. On the one hand the symbolic approach and on the other hand the numerical approach. The video then describes machine learning with its two main modes of operation: supervised learning and reinforcement learning. Deep learning (or deep neural network) is then discussed, which is one of the modalities of machine learning and is inspired by the functioning of the brain. In addition to the algorithms, which were mentioned at the beginning of the video, Guillaume reminds us of the importance of data. Because these data must be numerous, and correctly labelled, to allow AIs to learn correctly, and to make satisfactory predictions. This is an opportunity to come back to the \"manufacture\" of data and its challenges. In its last part, the video finally discusses the environmental impact of AI.","title":"Of Machine Learning and Data"},{"location":"module-2-how-does-ml-work/2-3-to-discover-of-machine-learning-data/2-3-1-where-does-the-risk-come-from.html","text":"Full article in French published on February 20, 2020 by binaire on Le Monde web site: \"D\u2019o\u00f9 vient le risque ? Des donn\u00e9es et des algorithmes\" - Authors: Serge Abiteboul, Thierry Vi\u00e9ville The meeting of researchers from the legal and computer science domains in the context of the Centre Internet et Soci\u00e9t\u00e9 (Internet and Society Center) launch was an opportunity for cross-reflection and to raise a number of questions and initial research approaches to be explored together. This article summarises the results of the round table. Serge Abiteboul, Thierry Vi\u00e9ville. Photo by Fernando Arcos from Pexels Classification of Risks Digital platforms and their role in society are occupying the media and governing bodies. We, lawyers and computer scientists, perceive them as new data markets. Several human actors, artists, authors, content creators, language developers, platform developers, application developers, Internet users and consumers, public and private actors, gravitate around these platforms and are exposed to two types of risk : Data risk is about data protection on these platforms. Algorithm-risk refers to the issues of algorithmic discrimination. This paper provides a first reflection on how to deal with digital platforms and data and algorithmic risks. These issues can be approached from two complementary points of view: the legal point of view, whose main concern is to define the frameworks that allow these risks to be identified and managed, and the IT point of view, whose aim is to develop the tools needed to quantify and resolve these risks. The three facets of algorithmic risk Algorithm risk can be characterised in three ways. Firstly, there is algorithmic confinement , which can also relate to opinions, cultural knowledge or even commercial practices. Indeed, the algorithms confront the Internet user with the same content, depending on his profile and the integrated parameters, despite the respect of the principle of fairness. This is the case on news recommendation sites such as Facebook or product recommendation sites such as Amazon. The second facet of algorithmic risk is linked to the control of all aspects of an individual's life , from the regulation of information for investors to his or her eating habits, hobbies, or even health status. This tracing of the individual suggests a form of surveillance that contravenes the very essence of individual freedom. The third is related to the potential violation of fundamental rights . In particular, algorithmic discrimination defined as unfavourable or unequal treatment, in comparison with other persons or other equal or similar situations, based on a ground expressly prohibited by law. This encompasses the study of the fairness ( fairness ) of ranking (sorting of people looking for a job online), recommendation, and prediction learning algorithms. The problem of discriminatory bias induced by algorithms concerns several areas such as online hiring on MisterTemp', Qapa and TaskRabbit, court decisions, police patrol decisions, or school admissions. From Risks to Bias in Data and in Algorithms We take up a classification of biases proposed by colleagues at T\u00e9l\u00e9com ParisTech and discussed in a report from the Institut Montaigne in Paris. We adapt this classification to risk-data and risk-algorithms with a focus on bias. \u00a9Sihem Amer-Yahia The data come from different sources and have multiple formats. They carry different types of bias. Data bias is mainly statistical. Data-bias is typically present in data values. For example, this is the case for a recruitment algorithm trained on a database in which men are over-represented will exclude women. Standard bias is a tendency to act in reference to the social group we belong to. For example, one study shows that women tend to click on job offers that they think are easier to get as a woman. Omitted variable bias (modelling or coding bias) is a bias due to the difficulty of representing or coding a factor in the data. For example, because it is difficult to find factual criteria to measure emotional intelligence, this dimension is absent from recruitment algorithms. The selection bias is in turn due to the characteristics of the sample selected to draw conclusions. For example, a bank will use internal data to derive a credit score, focusing on those who have or have not obtained a loan, but ignoring those who have never needed to borrow, etc. The algorithmic bias is mainly a matter of reasoning. An economic bias is introduced into algorithms, intentionally or unintentionally, because it will be economically efficient. For example, an advertising algorithm directs ads to particular profiles for which the chances of success are greater; razors will be more likely to be seen by men, fast food by socially disadvantaged populations, etc. A range of cognitive biases should also be mentioned Conformity biases, known as \"panhandle sheep,\" correspond to our tendency to reproduce the beliefs of our community. This is the case, for example, when we support a candidate in an election because his family and friends support him. Confirmation bias is a tendency to favour information that reinforces one's view. For example, after a trusted person has told us that so-and-so is bossy, we only notice examples that show this. The illusory correlation bias is a tendency to want to associate phenomena that are not necessarily related. For example, thinking that there is a relationship between oneself and an external event such as a train delay or weather. The endogenous bias is related to a relative inability to anticipate the future. For example, in the case of credit scoring , it may be that a prospect with a poor loan repayment history may change their lifestyle when they decide to start a family. \u00a9Sihem Amer-Yahia Algorithms are a series of instructions that manipulate input data and return output data. This input data sometimes carries biases. Biases can also be found in one or more instructions of the algorithms. Should we address data-risk and algorithm-risk on digital digital platforms together or separately? Let's consider two examples, the context of technology blockchain technology, and that of Artificial Intelligence systems. On blockchain, first of all, there are data, risks and their bias. Let's take the example of data and the associated risks. The blockchain works by encryption with two cryptographic keys: private keys and public keys. Many Internet users entrust platforms with their private keys, thus delegating to them the management of their address and the movement of funds. These private keys are stored either in a file accessible on the Internet ( hot storage ) or on an isolated device ( cold storage ). The former is obviously highly vulnerable to hacking, while 92% of trading platforms report using a cold storage system. Since 2011, 19 serious incidents have been recorded for an estimated loss amounting to $1.2 billion. The causes of these incidents are multiple. The most common comes from private key tampering, followed by the introduction of malware. The hack of the Coincheck platform in Japan in January 2018 illustrates the weak protection of the hot storage system. Another example on algorithms and associated risks, the exchange of cryptocurrencies on platforms is seeing the development and diversification of market infrastructures. The ambition is \"to enable the establishment of an environment that promotes the integrity, transparency and security of the services concerned for investors in digital assets, while ensuring a secure regulatory framework for the development of a robust French ecosystem\" . France has recently France has recently adopted a legal framework to regulate these activities activities in a flexible manner. However, at the global level, the risks of non-transparent listings or suspicious transactions of non-transparent quotations or suspicious transactions resembling of direct price manipulation or informed investor practices, such as type of frontrunning . Frontrunning is a stock market technique that allows a broker to use an order transmitted by its customers transmitted by his clients in order to enrich himself. The technique consists of The technique consists of taking advantage of price discrepancies generated by large orders The technique consists of taking advantage of price shifts caused by large orders placed by the broker's clients. Let's get to the question \"should we address risk-data and data-risk and algorithm-risk on digital platforms together or separately?\" With regard to blockchain, the law's answer is separate, because the risks captured are different. On the one hand, certain provisions of criminal law, civil liability or personal data protection will be mobilised. mobilised. On the other hand, in France, the recent legal framework for the other hand, in France, the recent legal framework aimed at capturing the activities of service providers and to avoid algorithmic risk is mainly regulatory. is mainly regulatory. Responsability vs. Accountability On AI systems, we will take the prism of liability and our question through the prism of liability and accountability. This question is diabolical because it requires the lawyer to dive into the world of computing in order to understand what artificial intelligence consists of, this catch-all word that covers, in reality, numerous computer sciences and techniques. And is it necessary to use this term at all, when the creator of the much used voice assistant Siri has just written a book with a slightly provocative title, which is a tad provocative, states that artificial intelligence intelligence does not exist... ( Luc Julia, L\u2019intelligence artificielle n\u2019existe pas, First editions, 2019- AI does not exist ) . A distinction between AI systems is often made: only certain systems are actually embedded in a body in order to offer its algorithmic behaviours: Other AI systems make algorithmic decisions or recommendations that can have an immediate effect on the real world and the human mind, without needing to be embodied in a body: These include consumer marketing recommendations, social network feeds, predictive justice, and are often seen as \"rigged\". However, all AI systems eventually become embedded in a machine: a robot, a car, a computer, a phone, and all AI systems have the potential to impact on the human mind or body, and even on personal rights ( M. Baccache, Intelligence artificielle et droits de la responsabilit\u00e9, in Droit de l\u2019intelligence artificielle, A. Bensamoun, G. Loiseau, (dir.), L.G.D.J., Les int\u00e9grales 2019, p. 71 f. - Law of Artificial Intelligence ) , so much so that we will choose here to grasp the question of liability when using AI systems in a transversal manner. The cross-cutting question we will ask is whether the specificity of AI systems, both in terms of their evolving nature and their complex governance, and in terms of the risks of their implementation for humans and society, are sufficiently well understood; volutive nature and their complex governance, as well as the risks of their implementation for human beings and society, do not call for a new approach; This is not to say that accountability, understood as the sole a posteriori sanction for the occurrence of a risk, should be seen as a complement between accountability in the governance of each AI system throughout its life cycle and a posteriori accountability. If accountability is recognised as a prerequisite for responsibility, it will imply considering data-risks and algorithmic-risks jointly, thus preserving the specificity of the system; cificity of each of these risks, but by linking them, because it is through the conjunction of these two types of risks, that consequences that are precarious for humans or society can occur. In fact, in its April 2019 guidelines on trustworthy AI, the High Level Expert Group on Artificial Intelligence, mandated by the European Commission to develop and implement a European strategy on artificial intelligence; In one of its proposals, the High Level Expert Group on Artificial Intelligence, mandated by the European Commission, recalls a fundamental point, namely the need to recognise and acknowledge that &some applications of AI may bring considerable benefits to the environment; While some AI applications may bring significant benefits to individuals and society, they may also have negative impacts, including impacts that may be difficult to anticipate, recognise or measure (e.g. in the form of the use of AI); (e.g. on democracy, the rule of law and distributive justice, or on the human mind itself) (High Level Expert Group on Artificial Intelligence, Guidelines for Trustworthy AI, April 2019, constituted by the European Commission in June 2018 ,). In doing so, the High Level Expert Group calls for appropriate measures to mitigate these risks where necessary, in a manner commensurate with the scale of the problem; (b) to take into account the extent of the risk and, on the basis of the articles of the Charter of Fundamental Rights of the European Union, to pay particular attention to situations involving more vulnerable groups of people; In this context, the European Commission should pay particular attention to situations concerning more vulnerable groups such as children, people with disabilities and other historically disadvantaged groups at risk of exclusion, and/or to situations characterised by asymmetries of power or information, for example between employers and workers, or between businesses and consumers. Even though certain risks and the protection of certain vulnerable groups require it, taking the appropriate measures is not easy, even beyond the current tension between the principle of innovation and the principle of security. The reason is that both the technical bricks used and the people involved in the operation of an AI system are numerous, varied and complex, leading to many interactions that are not easy to master. It is worth noting that the high-level panel makes a set of proposals, in terms of the ethics and technical robustness of AI systems, that challenge the idea that trust in an AI system is a prerequisite for its success; In the light of the current risks of AI deployment, trust in an AI system must be based on an a priori accountability of its governance throughout its life cycle, which includes the objective of making these actions explicit. The notion of accountability is central to understanding the complementarity and the long continuum between accountability and responsibility. More than the term responsibility, this notion of accountability can be precisely translated by the notions of accountability and/or responsibility. This accountability makes it possible to consider data-risks and algorithmic risks together, thus taking into account the specificity of each of these risks, but also to take into account the fact that each of them has its own specificity; It is through the conjunction of these two types of risks that consequences that are harmful to humans or society can occur. Summary In summary , the legal perspective will differ depending on the issues and applicable concepts. In the case of blockchain, it is important to separate the data risk from the risk-algorithms as they deal with different issues and require and require different legal frameworks. The The first deals with the issue of disclosure of the identity of the The first deals with the issue of disclosure of the identity of parties, which is a matter of data security, while the second deals with the issue of fraudulent digital assets. In the In the case of artificial intelligence systems, it will depend on whether whether the damage should be prevented or sanctioned once it has the damage or to sanction it once it has occurred. In the case of In the case of a search for accountability, it is necessary to consider the In the case of accountability, data-risks and algorithm-risks should be considered together. If the question is one of responsibility ( liability ) and accountability ( accountability ), i.e., that of imputing fault to a the issue of attributing fault to a natural person, it will be important to to separate the two risks. This separation is also the one that is This separation is also the one advocated in computer science in order to identify the \"culprits\": data or algorithms. Data provenance and algorithmic tracing techniques and algorithmic tracing techniques will help to isolate the the reasons for the fault. The first step is to identify whether the fault is due to a data risk such as disclosure of privacy or statistical bias in the data, or data, or to an algorithmic risk of the economic or cognitive type, or if the cognitive type, or whether the fault is due to both. It will therefore only be possible to fault and determine the applicable legal frameworks only if there is a separation. if there is a separation. Similarly, if the objective is to \"fix\" the data or the algorithm, the data or the algorithm, the two types of risk must be considered separately. be done separately. This is called orthogonality in computer science. According to the dictionary, the instruction set of a computer is said to be orthogonal when (almost) all instructions can be applied to all types of data. An orthogonal instruction set simplifies the task of the compiler since there are fewer special cases to deal with the operations can be applied as is to any type of data. any type of data. In our context, this would mean having a context, this would mean having a perfect dataset and seeing how the algorithm behaves to determine if there is a risk-algorithms and having a perfect algorithm and examining the results applied to a dataset to determine the risk-data. These strategies have a bright future ahead of them. Authors : Sihem Amer-Yahia (DR CNRS INS2I, Univ. Grenoble-Alpes) - Research director at National Centre for Scientific Research Am\u00e9lie Favreau (MdC Droit Priv\u00e9, Univ. Grenoble-Alpes) - Associate Professor in Private Law Juliette S\u00e9n\u00e9chal (MdC Droit Priv\u00e9, Univ. de Lille) -Associate Professor in Private Law","title":"Where Does the Risk Come From?"},{"location":"module-2-how-does-ml-work/2-4-quiz-module-2/2-4-0-quiz-2-mechanisms-in-ai.html","text":"Quiz Module 2: Mechanisms in AI Quiz Module 2 contains 3 parts: QUIZ 1: Mechanisms in AI QUIZ 2: Machine Learning methods QUIZ 3: AI and data Click on the image below and start the quiz!","title":"Mechanisms in AI"},{"location":"module-3-AI-at-our-service/3-0-introduction/3-0-0-presentation.html","text":"Objective of Module 3: Understanding the challenges and levers for putting AI to work for people. Content: To Question: Are Humans fit for the Scrap Heap? To Experiment: Let's Create with AI To Discover: Artificial Intelligence at our Service? Quiz Module 3: Humans and AI Investment Time : about 40 minutes","title":"Presentation"},{"location":"module-3-AI-at-our-service/3-1-to-question-are-humans-fit-for-the-scrap-heap/3-1-0-are-humans-fit-for-the-scrap-heap.html","text":"Today with computers we talk about automatic information processing. Does this mean that artificial intelligence could replace intellectual work? Do you also have questions? Watch the video below (1 minute 51): Synopsis The main character, Guillaume , calls the hotline after reading an article about the possible disappearance of many jobs, whose work would in the future be done by AIs. He mentions projections that, in 10 or 20 years, foresee the disappearance of many jobs, especially white-collar jobs, whose tasks could be automated. The Hotline puts this evolution into perspective and makes the link with the transformations that occurred in the 19 th century during the industrial revolution. She recalls that all innovation leads to the replacement of certain activities by others. She also emphasises that today it is not really possible to know exactly which jobs will be automated. The discussion between Guillaume and The Hotline then refocuses on a new question: not what activities can be performed by AIs but what decisions should or should not be made by them.","title":"Are Humans fit for the Scrap Heap?"},{"location":"module-3-AI-at-our-service/3-1-to-question-are-humans-fit-for-the-scrap-heap/3-1-1-a-revolution-that-affects-both-blue-and-white-collar-workers.html","text":"Previous industrial revolutions have always had an almost exclusive impact on the category of workers known as \" blue-collar workers \". That is, occupations involving rather manual tasks. In contrast, the so-called \" white-collar workers \", those in \"office\" jobs, have never before been threatened by these revolutions. The technological revolution heralded by AI is in this respect very different from previous ones. It threatens both white and blue collar workers. In the list of jobs that will be most affected in the years to come, we find not only jobs such as train driving or counter work, but also in banking and insurance, in accounting, in office automation or in management. In an interview with Chut magazine , Erwan Tison, director of studies at the Sapiens Institute, identifies in his book Robots, my job and me several job categories that, to different degrees, could be transformed by AI. Image by Thomas Meier for Pixabay \"Physical\" jobs could be impacted by robotisation. Professionals would be replaced by machines that can perform difficult tasks faster and more efficiently than they can. \" Intellectual \" jobs would be impacted by digitalisation. Algorithms could perform systematic, tedious and time-consuming tasks faster than humans. Some professionals would see their daily lives improved by artificial intelligence mechanisms, which would allow them to be relieved of repetitive tasks to focus on the most important aspects of their jobs. Finally, some jobs, which require too much complex interaction with the environment, would not be directly threatened. This is the case for gardeners or plumbers, among others. Are these predictions likely and should we fear that we will eventually be mostly replaced by robots in our work? For the researcher and economist Gregory Verdugo \"Very poor indeed who can predict what will be technically automatable.\" We could seek to automate our entire society, but that would be our choice and we have to ask ourselves the question of the purpose of such a choice. For the moment, AI mainly allows us to facilitate the work of certain professionals, but in the future it will depend on how we collectively choose to organise society with these new Tools. Image by Seanbatty from Pixabay Moreover, if artificial intelligence is capable of learning quickly, there are human capacities that can only be simulated and not realised, such as human empathy and creativity, for example. Yet in many professions, these two qualities are indispensable. A future where our nurses, artists or educators resemble R2D2 is therefore not yet possible.","title":"A revolution that affects both blue and white collar workers"},{"location":"module-3-AI-at-our-service/3-2-to-experiment-lets-create-with-ai/3-2-0-lets-create-with-ai.html","text":"Detecting AI: AI or Human? The following activity is about identifying whether an image is real or produced by an AI. This experimental activity is an introduction to understand GANs (Generative Adversarial Networks). Want to try? Click on the image below and let us guide you!","title":"Let's Create with AI"},{"location":"module-3-AI-at-our-service/3-2-to-experiment-lets-create-with-ai/3-2-1-generative-adversarial-networks.html","text":"Generative Adversarial Networks fall between supervised learning based on the provision of input data, the corresponding desired output of which is known in order to estimate the input-output relationship beyond the samples provided for training, and unsupervised learning. When only input data is available, to discover certain structures within the data (e.g. the number of parameters that characterise it), there are many other paradigms, for example semi-supervised where we mix data where we know the desired output and others do not, in order to mix the two approaches. Another so-called self-supervised paradigm consists of, from input data, finding an external mechanism to generate the corresponding outputs. This is to save the enormous human effort of entering for each input the desired output, e.g. labelling images by hand, pixel by pixel if necessary (if one wants to find where the cat is in an image). For example, to learn to automatically colourise images, one can start with colour images, reduce them to black and white images, and then train the mechanism by providing it with the desired colour images, here known without the need to reconstruct them for each black and white image. This also works for learning the relative position of elements in an image that is being cropped, or temporal consistency in a video. But it doesn't work for everything. It works whenever you find a trick to automatically generate the desired inputs and output from the data. This is sort of unsupervised learning that auto-generates data for a supervised learning paradigm.","title":"Generative Adversarial Networks (GAN)"},{"location":"module-3-AI-at-our-service/3-3-to-discover-artificial-intelligence-at-our-service/3-3-0-artificial-intelligence-at-our-service.html","text":"The development of computer science in general and artificial intelligence in particular raises ethical questions that are very different from those posed, since Hippocrate, by the development of life sciences and medicine. Traditional values, such as respect for privacy, transparency, intelligibility... are to be rethought. Want to discover \"Artificial intelligence at our service?\" View the video below (4'36\"): Synopsis The main character, -Guillaume-, reminds us of the challenges of AI in a very wide range of fields. He emphasises that this reflection on artificial intelligence also leads us to reflect on human intelligence. And this reflection leads to a questioning of humanity and our role as citizens in society. The discussion with The Hotline then focuses on the notion of creativity and its unique link with human intelligence. And just as AIs cannot create but only imitate, they cannot feel emotions but only imitate the manifestations of our emotions. It is then stated that AIs are valuable decision support tools, in a wide variety of fields. However, these AIs can have flaws, and it is important that we know what they contain (i.e. which algorithms and which data) which is not so easy. This observation leads Guillaume and The Hotline to end their discussions on the conditions of a trustworthy AI and its stakes in our lives as human beings and citizens.","title":"Artificial Intelligence at our Service?"},{"location":"module-3-AI-at-our-service/3-3-to-discover-artificial-intelligence-at-our-service/3-3-1-can-ai-be-genderbiased-can-ai-be-sexist.html","text":"Translated text from the french IAI Mooc. Photo: CBC/Radio-Canada -iStock - Editing: Amarilys Proulx An artificial intelligence does not think. So can it be racist, sexist or homophobic? It is possible for it to reproduce discriminatory mechanisms, if the mechanism has been trained by biased data. Human beings havecognitive biases in their reasoning. Cognitive biases are the set of psychological, moral, social, cultural... factors that influence, without our realizing it, our thinking mechanisms. These cognitive biases have an impact on logical thinking, as they influence our ways of thinking and lead us to make decisions guided by factors that are not rational. They can then be the cause of discriminatory decisions. But it is we who provide data to the algorithms. If there are biases in the data provided, this will be reflected in the results. For example, the company Amazon has standard recruitment needs, and to make it easier to manage them, it decided to use an artificial intelligence capable of sorting CVs. In order for this AI to learn how to sort profiles, Amazon provided all the recruitment data it had recorded between 2004 and 2014. However, after a year, Amazon realised that the AI was systematically rejecting female profiles for technical and IT positions. The artificial intelligence, had simply determined by analysing the data it had been provided, that the recruitment for these positions over the last ten years was largely male 1 . Amazon stopped this artificial intelligence because the results obtained had gender biases. But it's not just biases in the data, there are also biases in the way the algorithms themselves are implemented. Technologies are therefore not neutral; they indirectly reflect the ways in which their designers and our societies operate. Even factors such as language can have an influence on them. In France, the composition of the language favours gendered learning more than in other countries. As names are predominantly masculine, an algorithm knows more masculine names than feminine ones. Thus, translation sites, such as Google translation, tend to give a masculine translation of names that are gender-neutral in another language 2 . Artificial intelligence can reinforce inequities as well as combat them. Faced with these ethical and technical challenges, several modes of action are envisaged. For example, guaranteeing more gender diversity in computer sciences, so that men are no longer over-represented. Or to overcome cognitive biases by teaching the algorithm to recognise them. This second option requires designers to consider cognitive biases upstream, even though they are not always aware of them. The best solution is to ensure diversity in the learning data, as data can be easier to change than mindsets. 3 1 Amazon a d\u00fb se d\u00e9barrasser d\u2019une intelligence artificielle sexiste\u201d, Slatefr,10 octobre 2018. ; Amazon met fin \u00e0 une intelligence artificielle sexiste\u201d par H\u00e9l\u00e8ne Chevallier, France Inter, emission c'est d\u00e9j\u00e0 demain, vendredi 12 octobre 2018 ; Amazon a du d\u00e9brancher un logiciel de recrutement qui s\u2019est r\u00e9v\u00e9l\u00e9 sexiste\u201d , La matinale d'Europe 1, le 6h - 9h, l'\u00e9dito \u00e9conomique d'Axel de Tarl\u00e9, le 12 octobre 2018. 2 Quand l\u2019Intelligence Artificielle rencontre les Sciences du Langage\u201d par Aurore Bisicchia, Chut!, janvier 2005. 3 \u201cL\u2019IA est\u2013elle sexiste, elle aussi ?\u201d par Anne-Marie Kermarrec, Blog Binaire, Le Monde, 08 mars 2018. \"Les biais sexistes de l'IA peuvent \u00eatre corrig\u00e9s, selon les chercheuses Aude Bernheim et Flora Vincent\" , L'Usine Nouvelle, propos recueillis par Marion Garreau, 08/03/2019. \"Le secteur de l\u2019intelligence artificielle est aussi masculin qu\u2019un bar des sports le soir d\u2019un match de Ligue 1\u201d , Interview d'Aude Bernheim et Flora Vincent par Ir\u00e8ne Inchausp\u00e9, L\u2019opinion, 08 Mars 2019.","title":"Can AI be Gender-biased?"},{"location":"module-3-AI-at-our-service/3-3-to-discover-artificial-intelligence-at-our-service/3-3-2-machine-learning-seeks-its-ethics.html","text":"Text translated from the online press article in French (01net.com) about ethics and machine learning. More and more artificial intelligence algorithms are beginning to rule our lives, although it is not clear whether their decisions are well-founded. Good practices and tools are beginning to emerge to promote the ethical and responsible use of this technology. Photo by Mike MacKensie from www.vpnsrus.com Whenever a new revolutionary technology has appeared, ethical and legal questions have quickly arisen: What are its limits of use? How can we ensure that it does not have harmful consequences for users? Who is responsible if something goes wrong? Artificial intelligence is no exception to this rule. Indeed, the question of ethics is particularly important given the impact of this technology - and in particular its variant of machine learning - on ever more aspects of our daily lives. Artificial intelligence algorithms decide what we see on the Internet, assess our creditworthiness, co-pilot our planes and cars, suggest products for purchase, help doctors detect our illnesses and prescribe medication, can recognise our voice and face at home and elsewhere, etc. Full article from 01net.com web site.","title":"Machine Learning Seeks its Ethics"},{"location":"module-3-AI-at-our-service/3-3-to-discover-artificial-intelligence-at-our-service/3-3-3-ai-ethics-in-the-future.html","text":"Text translated from the online press article in French (www.lemonde.fr) about ethics and machine learning. Artificial Intelligence (AI) is at the heart of many debates and controversies in society. No area of social and economic life seems to be spared from the subject. What is interesting to note is that in all the debates today around AI, ethics is always called upon. No one seems to reduce the question of AI to a simple technical issue. This is evidenced by the proliferation of reports on the ethics of AI, produced by private companies, public actors and civil society organisations. Among the reports, the statements may differ somewhat, but they all seem to subscribe to the same form of ethical imperative. This is the injunction to anticipate the impacts of technologies. Ethics - to use the words of the philosopher Hans Jonas - must today become \"ethics of the future\". Anticipating the impacts of the development of AI therefore becomes an ethical imperative. Such an imperative is not new. It is notably at the heart of the \"responsible research and innovation\" approach advocated by the European Commission. It seems to me that such an analogous concept of responsibility is also at the heart of recent French reports on AI. I will focus here on three recent reports: the CNIL report 1 , the Villani report 2 , and the CERNA report 3 . Let's start with the Villani report. This report states that: \"the law cannot do everything, among other things because the time of the law is much longer than that of the code. It is therefore essential that the \"architects\" of the digital society (...) take their fair share of this mission by acting responsibly. This implies that they are fully aware of the possible negative effects of their technologies on society and that they actively work to limit them. The movement is twofold: anticipating the downstream side of technological development, in order to - upstream - modify the design to prevent negative ethical impacts. To support this approach, the Villani report envisages obliging AI developers to carry out a discrimination impact assessment in order to \"force them to ask the right questions at the right time\". The CNIL report also contains a similar recommendation: \"Work on the design of algorithmic systems in the service of human freedom\". Anticipate to integrate ethics as early as possible in the technological development process. The desire to implement \"ethics in the future\" is quite commendable, but many questions remain unanswered which I would like to review. Can the future be anticipated? First of all, there is an epistemological criticism. How can we ethically judge the potentialities opened up by digital technology? Faced with such a development, are we not plunged into what economists call 'radical uncertainty'? The possibilities opened up by \"Big Data\" are a good example of this radical uncertainty. This term refers to the phenomenon of the uninterrupted and exponential proliferation of data. This development has led to an evolution in the technical language used to measure the storage power of data. From bytes, we have moved on to megabytes, gigabytes, etc. As Eric Sardin points out, with units of measurement such as the petabyte, the zeta-byte or the yottabyte, it is clear that we are dealing with units of measurement that purely and simply exceed our human structures of intelligibility 4 . Moreover, don't all the reports insist on the unpredictability of certain learning algorithms? Aware of this unpredictability, the CNIL defends in its report a principle of \"vigilance\" which institutes the obligation of continuous ethical reflection. But more fundamentally, should we not recognise that not everything can be anticipated? Ian-S on Visualhunt.com / CC BY-NC-ND A 'colonised' future? It is also important to realise that this future that we are enjoined to anticipate is saturated with fears, expectations and promises. The future is not an empty time, but a time constructed by scenarios, road-maps and prophetic discourses. To use Didier Bigo's expression, the future is 'colonised' by numerous actors who try to impose their vision as a common matrix for all anticipation of the future. Initially, he coined this expression in the context of a reflection on surveillance technologies, to designate the claims and strategies of experts who apprehend the future as a \"future before, as a future already fixed, a future whose events they know\" 5 . The robotic ethic runs the same risk of colonising the future, as Paul Dumouchel and Luisa Damiano illustrate in their book Living with Robots. The latter are thinking in particular of authors such as Wallach and Allen, in Moral Machines, Teaching Robots Right from Wrong, who propose a programme to teach robots the difference between right and wrong, to make them \"artificial moral agents\". But isn't 'programming' a morally autonomous agent a contradiction in terms? I do not intend to enter into a metaphysical debate on this subject. Rather, I would like to point out that such a programme has literally 'colonised' the horizon of expectation of debates on robotic ethics. Let us follow Dumouchel and Damiano to grasp this point. They note that, in the opinion of some of the protagonists of robotic ethics, \"we are still far from being able to create autonomous artificial agents that could be true moral agents. We don't even know if we will ever be able to do that,\" they say. We don't know if such machines are possible\" 6 . One of the answers put forward by the proponents of robotic ethics is then that \"we should not wait to elaborate such rules until we are caught unawares by the sudden irruption of autonomous artificial agents. It is important already to prepare for an inevitable future. Philosophers must already participate in the development of the robots that will populate our daily lives in the future by developing strategies to inscribe moral rules in robots that constrain their behaviour\" 7 . This theme of the inevitable empowerment of machines is powerful and quite problematic. Of all the possibilities, attention is focused on this scenario, which is posited as 'inevitable'. Such a focus of attention poses several problems. On the one hand, it raises epistemological questions: why is it possible to support this inevitability? On the other hand, for Dumouchel and Damiano, this belief, although not rational, has certain real effects: it diverts attention from power issues that are already at stake, namely that the empowerment of robots, the fact of delegating decisions to them and letting them choose for themselves, may mean the loss of decision-making power for a few, but it also intensifies the concentration of decision-making in the hands of a few (programmers, robot owners, etc.). Image form the film Forbiden Planet (USA, 1956) \u2013 MGM productions Who anticipates? A related question is who will do this anticipation of ethical impacts? Who will be the perpetrators of the discriminatory impacts? Is it only researchers? On this point, all the reports make it clear that this ethical imperative to anticipate does not only concern researchers. The CERNA report specifies that \"researchers must deliberate from the outset of their project with the persons or groups identified as being likely to be influenced\". For the Villani report, \"We must create a real forum for debate, plural and open to society, in order to determine democratically what AI we want for our society\". As for the CNIL, it states in its report that \"Algorithmic and artificial intelligence systems are complex socio-technical objects, shaped and manipulated by long and complex chains of actors. It is therefore all along the algorithmic chain (from the designer to the end user, through those who train the systems and those who deploy them) that action must be taken, using a combination of technical and organisational approaches. Algorithms are everywhere: they are therefore everyone's business\". The reports cited do pay attention to the question of WHO anticipates. However, the injunction to deliberate with \"individuals or groups identified as potentially being influenced\" (CERNA) is not enough. Determining the list of people concerned must be an object of reflection and research, not a prerequisite for this anticipation process. Indeed, anticipating these impacts can make us aware of new stakeholders to be included in the reflection. Moreover, what form should be given to an ethics of AI that is \"everybody's business\"? How can it be instituted? A consideration of time that obscures space Finally, a last question left unanswered is the privilege given to time over space in ethical reflection. The development of digital technology, and in particular AI, has contributed to the idea that the virtualisation of exchanges would reduce distances and the importance of places. Such a belief is, for example, at the heart of the development of telemedicine: a chronically ill person can be monitored indifferently at home or in an institution, a specialist can be called upon by tele-expertise regardless of the distance separating him from his colleague, etc. However, as the sociologists Alexandre Mathieu-Fritz and G\u00e9rald Gaglio state, \"telemedicine does not lead to the abolition of borders and spaces, contrary to the common sense view often implicitly conveyed by public policies\" 8 . In order to be effective, telemedicine requires a certain amount of spatial planning. In an ethnographic study carried out with telemonitored patients, Nelly Oudshoorn has shown the extent to which places, domestic space and public space influence and shape the way in which technologies are implemented, just as, conversely, these technologies literally transform these spaces. The home thus becomes a hybrid place, a medicalised living space. This spatial dimension does not really seem to be taken into account. However, an author such as Pierre Rosanvallon reminded us ten years ago in his book La l\u00e9gitimit\u00e9 d\u00e9mocratique that the legitimacy of public action today depends more and more on what he calls a 'principle of proximity', an attention to local contexts. This is demonstrated by the promotion of an \"experimentation\" approach in the field of AI in health: \"in order to benefit from the advances of AI in medicine, it is important to facilitate experimentations of AI technology in health in real time and as close as possible to the users, by creating the necessary regulatory and organisational conditions\" (Villani Report). In a more inductive way and starting from the field, it would be a question of favouring a new construction of public action: \"experimenting with public action\" as Cl\u00e9ment Bertholet and Laura L\u00e9tourneau invite us to do 9 . While this experimental approach invites us to take into account local contextual realities, the objective is always 'scalability' (the fact that it can be used at different scales). I take this term from the anthropologist Anna Lowenhaupt Tsing, who defines it as: \"the capacity of projects to expand without changing the framework of their hypothesis\" 10 . It is true that we experiment locally, but it seems to me that we do so in order to identify what can be generalised and made operational in an undifferentiated way at different scales. Can a remote monitoring system - to take this example - be applied in any context? Can all living spaces become places of care? Does the ethics of AI take into account the fact that the impacts of AI will differ according to places and spaces? Where should ethical impact assessments take place? Should they be centralised or localised in the places where technical objects are designed? More fundamentally, is it possible to determine these impacts without moving? On this point, the mapping of the global landscape of AI ethics carried out by A. Jobin et al 11 is quite instructive. They conducted a comparative analysis of 84 reports on AI ethics. These are documents produced by government agencies, private firms, non-profit organisations and learned societies. Their analysis highlights the fact that the majority of reports are produced in the USA (20 reports), the European Union (19), followed by the UK (14) and Japan (4). African and Latin American countries are not represented independently of international or supra-national organisations. Does this geographical distribution not indicate that this spatial dimension is being overlooked? Alain Loute (Centre d\u2019\u00e9thique m\u00e9dicale, labo ETHICS EA 7446, Universit\u00e9 Catholique de Lille - Centre for Medical Ethics /Catholic University of Lille) 1 ( https://www.lemonde.fr/#_ftnref1 ) Comment permettre \u00e0 l\u2019homme de garder la main ? Les enjeux \u00e9thiques des algorithmes et de l\u2019intelligence artificielle, CNIL, 2017 (How to enable man to keep his hand in, in.The ethical challenges of algorithms and artificial intelligence). 2 ( https://www.lemonde.fr/#_ftnref2 ) Donner un sens \u00e0 l\u2019intelligence artificielle, Pour une strat\u00e9gie nationale et europ\u00e9enne , Rapport Villani, 8 mars 2018 (Giving meaning to artificial intelligence, For a national and European strategy). 3 ( https://www.lemonde.fr/#_ftnref3 ) Ethique de la recherche en apprentissage machine , Avis de la Commission de r\u00e9flexion sur l\u2019Ethique de la Recherche en sciences et technologies du Num\u00e9rique d\u2019Allistene (CERNA), juin 2017 (Ethics of machine learning research, in.Opinion of the Allistene Commission on Research Ethics in Digital Science and Technology). 4 E. Sardin, La vie algorithmique, Critique de la raison num\u00e9rique, Ed. L\u2019\u00e9chapp\u00e9e, Paris 2015, pp. 21-22 (Algorithmic Life, Critique of Digital Reason). 5 D. Bigo, \u00ab S\u00e9curit\u00e9 maximale et pr\u00e9vention ? La matrice du futur ant\u00e9rieur et ses grilles \u00bb, in B. Cassin (\u00e9d.), Derri\u00e8re les grilles, Sortons du tout-\u00e9valuation, Paris, Fayard, 2014, p. 111-138, p. 126 (Maximum security and prevention? The future tense matrix and its grids). 6 P. Dumouchel et L. Damiano, Vivre avec des robots, Essai sur l\u2019empathie artificielle, Paris, Seuil, 2016, p. 191 (Living with robots, An essay on artificial empathy). 7 ibid., p. 191-192. 8 A. Mathieu-Fritz et G. Gaglio, \u00ab \u00c0 la recherche des configurations sociotechniques de la t\u00e9l\u00e9m\u00e9decine, Revue de litt\u00e9rature des travaux de sciences sociales \u00bb, in R\u00e9seaux, 207, 2018/1, pp. 27-63 (In search of the socio-technical configurations of telemedicine). 9 C. Bertholet et L. L\u00e9tourneau, Ub\u00e9risons l\u2019Etat avant que les autres ne s\u2019en chargent, Paris, Armand Collin, 2017, p.182 (Let's uberise the state before others do it). 10 A. Lowenhaupt Tsing, Le champignon de la fin du monde, Sur la possibilit\u00e9 de vivre dans les ruines du capitalisme, Paris, La D\u00e9couverte, 2017, p. 78(The mushroom at the end of the world, On the possibility of living in the ruins of capitalism). 11 A. Jobin, M. Ienca, & E. Vayena, \u00ab The global landscape of AI ethics guidelines \u00bb, in Nat Mach Intell, 1, 2019, pp. 389\u2013399.","title":"AI Ethics \"in the future\""},{"location":"module-3-AI-at-our-service/3-4-quiz-module-3/3-4-0-quiz-3-humans-and-ai.html","text":"Quiz Module 3: Humans and AI Quiz Module 3 contains 3 parts: QUIZ 1: AI & Neural Networks QUIZ 2: Delegating Tasks to Algorithms QUIZ 3: Bias and Creativity Click on the image below and start the quiz!","title":"Humans and AI"}]}